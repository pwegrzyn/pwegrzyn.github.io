<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ml on Patryk Węgrzyn</title><link>https://pwegrzyn.github.io/categories/ml/</link><description>Recent content in ml on Patryk Węgrzyn</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 15 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://pwegrzyn.github.io/categories/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Loss Landscapes, Mode Connectivity and Generalization in Deep Neural Nets</title><link>https://pwegrzyn.github.io/2021/mode-connectivity/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://pwegrzyn.github.io/2021/mode-connectivity/</guid><description>The impact of deep learning on the fields of computer science, artificial intelligence and many others cannot be understated. Some even go as far as to say that this type of automated reasoning based on data and not on explicit rules will spark a new era of software [1]. However, despite the tremendous success of deep neural nets, their remarkablew effectivness and good generalization capabilites are not yet fully understood. Over the last couple of months I decided to dig a bit deeper into the fundamental aspects of deep learning such as network architecture, depth, width, optimizer method, loss surface and others, to try to develop an intuition on what makes some models great and others mediocre.</description></item></channel></rss>