<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=content-type content="text/html"><meta name=viewport content="width=device-width,initial-scale=1"><title itemprop=name>Loss Landscapes, Mode Connectivity and Generalization in Deep Neural Nets | Patryk Wegrzyn</title>
<meta property="og:title" content="Loss Landscapes, Mode Connectivity and Generalization in Deep Neural Nets | Patryk Wegrzyn"><meta name=twitter:title content="Loss Landscapes, Mode Connectivity and Generalization in Deep Neural Nets | Patryk Wegrzyn"><meta itemprop=name content="Loss Landscapes, Mode Connectivity and Generalization in Deep Neural Nets | Patryk Wegrzyn"><meta name=application-name content="Loss Landscapes, Mode Connectivity and Generalization in Deep Neural Nets | Patryk Wegrzyn"><meta property="og:site_name" content="Patryk Wegrzyn"><meta name=description content="Software Engineer ðŸ’»"><meta itemprop=description content="Software Engineer ðŸ’»"><meta property="og:description" content="Software Engineer ðŸ’»"><meta name=twitter:description content="Software Engineer ðŸ’»"><meta property="og:locale" content="en-gb"><meta name=language content="en-gb"><meta itemprop=image content="https://pwegrzyn.github.io"><meta property="og:image" content="https://pwegrzyn.github.io"><meta name=twitter:image content="https://pwegrzyn.github.io"><meta name=twitter:image:src content="https://pwegrzyn.github.io"><meta property="og:type" content="article"><meta property="og:article:published_time" content="2021-08-15T00:00:00Z"><meta property="article:published_time" content="2021-08-15T00:00:00Z"><script defer type=application/ld+json>{"@context":"http://schema.org","@type":"Article","headline":"Loss Landscapes, Mode Connectivity and Generalization in Deep Neural Nets","author":{"@type":"Person","name":""},"datePublished":"2021-08-15","description":"","wordCount":1642,"mainEntityOfPage":"True","dateModified":"2021-08-15","image":{"@type":"imageObject","url":""},"publisher":{"@type":"Organization","name":"Patryk Wegrzyn"}}</script><meta name=generator content="Hugo 0.122.0"><link rel=canonical href=https://pwegrzyn.github.io/posts/mode-connectivity/><link href=https://pwegrzyn.github.io/style.min.d43bc6c79baa87f006efb2b92be952faeedeb1a3ab626c1d6abda52eae049355.css rel=stylesheet><link href=https://pwegrzyn.github.io/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css rel=stylesheet><link rel=apple-touch-icon sizes=180x180 href=https://pwegrzyn.github.io/icons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://pwegrzyn.github.io/icons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://pwegrzyn.github.io/icons/favicon-16x16.png><link rel=mask-icon href=https://pwegrzyn.github.io/icons/safari-pinned-tab.svg><link rel="shortcut icon" href=https://pwegrzyn.github.io/favicon.ico><link rel=manifest href=https://pwegrzyn.github.io/site.webmanifest><meta name=msapplication-config content="/browserconfig.xml"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#434648"><link rel=icon type=image/svg+xml href=https://pwegrzyn.github.io/icons/favicon.svg></head><body data-theme=dark class=notransition><script src=https://pwegrzyn.github.io/js/theme.min.8961c317c5b88b953fe27525839672c9343f1058ab044696ca225656c8ba2ab0.js integrity="sha256-iWHDF8W4i5U/4nUlg5ZyyTQ/EFirBEaWyiJWVsi6KrA="></script><div class=navbar role=navigation><nav class=menu aria-label="Main Navigation"><a href=https://pwegrzyn.github.io/ class=logo><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-home"><title>Home</title><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
</a><input type=checkbox id=menu-trigger class=menu-trigger>
<label for=menu-trigger><span class=menu-icon><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentcolor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7H3.40726"/><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488H3.49301"/><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"/><path stroke-linecap="round" stroke-linejoin="round" d="M.5 12.5V1.5c0-.552285.447715-1 1-1h11C13.0523.5 13.5.947715 13.5 1.5v11C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C.947715 13.5.5 13.0523.5 12.5z"/></svg></span></label><div class=trigger><ul class=trigger-container><li><a class=menu-link href=https://pwegrzyn.github.io/>Home</a></li><li><a class="menu-link active" href=https://pwegrzyn.github.io/posts/>Posts</a></li><li><a class=menu-link href=https://pwegrzyn.github.io/pages/about/>About</a></li><li class=menu-separator><span>|</span></li></ul><a id=mode href=#><svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1"><title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1=".5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1=".5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/></g></svg><svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1"><title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1=".5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1=".5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/></g></svg></a></div></nav></div><div class="wrapper post"><main class=page-content aria-label=Content><article><header class=header><h1 class=header-title>Loss Landscapes, Mode Connectivity and Generalization in Deep Neural Nets</h1><div class=post-meta><time datetime=2021-08-15T00:00:00+00:00 itemprop=datePublished>15 Aug 2021</time></div></header><details class=toc zgotmplz><summary><b>Table of Contents</b></summary><nav id=TableOfContents><ul><li><a href=#loss-landscape-visualization>Loss landscape visualization</a></li><li><a href=#structural-symmetries-of-neurons>Structural symmetries of neurons</a></li><li><a href=#mode-connectivity>Mode connectivity</a></li><li><a href=#loss-geometry-in-the-context-of-generalization>Loss geometry in the context of generalization</a></li><li><a href=#stochastic-weight-averaging>Stochastic weight averaging</a></li><li><a href=#bonus-depth-vs-width>Bonus: depth vs width</a></li><li><a href=#references>References</a></li></ul></nav></details><div class=page-content><p>The impact of deep learning on the fields of computer science, artificial intelligence and many others cannot be understated. Some even go as far as to say that this type of automated reasoning based on data and not on explicit rules will spark a new era of software <a href=#1>[1]</a>. However, despite the tremendous success of deep neural nets, their remarkablew effectivness and good generalization capabilites are not yet fully understood. Over the last couple of months I decided to dig a bit deeper into the fundamental aspects of deep learning such as network architecture, depth, width, optimizer method, loss surface and others, to try to develop an intuition on what makes some models great and others mediocre. Below I present my key observations and useful resources on this matter.</p><h2 id=loss-landscape-visualization>Loss landscape visualization</h2><p>Visualizing the loss function surfaces can be a surprisingly helpful way of measuring and predicting future model performance and generalization capabilities. Unfortunately, this tasks is actually easier said than done in practice. Taking a multi-million parameter model and representing in an insightful way on a 2D/3D plot without artifacts related to dimensionality reduction is quite challanging. Some initial attempts have been proposed by Goodfellow et al. <a href=#3>[3]</a>. In this approach, we choose two different minima and linearly interpolate the loss surface between them, producing a 2D plot of the loss function. This methods works, but has some major issues. The biggest problem with it is that it ignores the scales of parameters on different parts of the network. A solution to this has been proposed by Li et al. <a href=#4>[4]</a>. Here, the authors added a normalization step: before plotting the values of the function on a parameter mesh grid, we first scale all parameters so that they absolute values have similar scales. This effectively reduces harmful artifacts in the visualization, thus producing a very robust method of investigating the geometrical structures of loss surfaces. Using this approach, the authors showed some very interesting observations: (1) very deep networks seem to have chaotic and non-convex loss landscapes; (2) the application of skip (residual) connections smooths the loss surfaces; (3) most network provably good network architectures produce highly convex loss landscapes.</p><h2 id=structural-symmetries-of-neurons>Structural symmetries of neurons</h2><p>For many years it has been speculated that the inherent symmetries in network architectures should give rise to many semi-equivalent minima in the loss function. The reasoning behind this conjecure goes something like this: picture two neurons within a single layer in a deep network. These neurons have some impact on the final ouput of the network. Now try a different permutation of them, i.e. swap their weights. Clearly the output of the network should remain the same, even though we changed the model structure. This means that these two permutations should correspond to two minima in the loss function which are closely related to each other. What do I mean by <em>related</em>? Imagine that instead of a discrete swap, we change the neurons&rsquo; weights continuously: we gradually decrease weights of one neuron and increase the weights of the other one. At some point during this process, the weights will have the same value. Finally, we continue the process untill the weights have their values fully swapped. How would this transformation affect the loss surface? It turns out that, staring from a given minimum, the continuous swap would take us through a low loss <em>valley</em>. Somewhere in the middle of the valley, directly in the point corresponding to the neurons having equal weights, we encounter a saddle point, and then finally, continuing along the valley, we reach its end - a second minimum representing the neuron weights being fully swapped <a href=#2>[2]</a>. THis leads us straight to the next point - mode connectivity.</p><h2 id=mode-connectivity>Mode connectivity</h2><p><em>Mode connectivity</em> refers to the observed phenomenon of different minima being connected by low loss paths. One of the first methods of finding these paths have been proposed by Garipov et al. <a href=#5>[5]</a>. THis is achieved by modifying the loss function to include a part maximizing a randomly sampled likelihood of a line segment connecting two minima. These can be used to find simple linear paths or more complex structures such as Bezier curves. The author&rsquo;s also defined an ensembling method based on this approach: we find a minimum for aboutr 80% of the computational budget, then we used a cycling cosine learning rate scheduler to move the minimum slightly along the low loss path, and we snapshot the models at certain time steps to produce the final ensemble. Following the work of Garipov, others continued to evolve the theorem of mode connectivity. Fort et al. <a href=#6>[6]</a> proposed that these low loss paths are in fact parts of larger submanifolds of constant loss present in the loss landscape, suggesting that the richness of mode connectivity is much broader than first expected. Benton et al. <a href=#7>[7]</a> further established that finding these manifolds is quite easy and, based on a simplex-finding algorithm, proposed a novel ensembling method which achieves SotA results. All this is to say that one should not think of the task of training deep learning models as a tedious process of finding a needle (single global minimum) in a haystack (entire parameter space), but rather as a relatively simple job of finding a <em>good-enough</em> partial minimum from a vast submanifold of low loss plateaus.</p><h2 id=loss-geometry-in-the-context-of-generalization>Loss geometry in the context of generalization</h2><p>Many studies have showed that the geometrical structures present in loss landscapes are stongly correlated with test performance and overall generalization potential. One of the main takeaways here is that wide minima are in general better than sharp minima, and that non-convex landscapes (containing many different sharp minima) are practically impossible to train. A fanstatic and intuitive explanation to this phenomenon can be seen in a recent talk by Leo Dirac (yes, he&rsquo;s a relative to THE Paul Dirac!) available currently on YouTube <a href=#8>[8]</a>. Some other great resources on this subject are these two talks by Tom Goldstein (also available on YT) <a href=#9>[9]</a> <a href=#10>[10]</a>. In short, a wide minium rougly corresponds to a wide margin classifier (think of a SVM which maximizes the distance from the classification line to certain data points), which by its nature has smaller variance and better test performance. This is good news for us, since we know that deep nets&rsquo; loss landscapes contain large low loss plateaus originating from neuron symmetries and mode connectivity. This, in turn, explains why deep learnig is so effective: the architecture of the model, by its nature, makes the training problem an easy task in practice, and also usually it leads to high-margin classifiers corresponding to models with smaller variance, which benefits generalization.</p><h2 id=stochastic-weight-averaging>Stochastic weight averaging</h2><p>Several independent research studies have reported that SGD tends to converge to boundaries of low loss regions, and not to the exact centers of minima <a href=#11>[11]</a>. Moreover, it has been seen that it converges to cycles around plateaus. This is understandable: as soon as SGD reaches the flat region, gradient vanishes and there&rsquo;s no more a &ldquo;force&rdquo; pulling the optimizer towards the center of the plateau, so instead it continues to loop around its boundary. In the same aricle, the authors proposed a method to bias SGD towards the center, because (and here&rsquo;s comes the brilliant observation) convergence to the center of the minimum would make the model less susceptible to perturbations in the loss landscape during test set inference. A simpler and arguably more elegant solution this this conundrum has been proposed by Izmailov et al. <a href=#12>[12]</a>: instead of biasing the path SGD takes, we just take snapshots of different points vanilla SGD encounters during the ending phases of training. Then we simply take the average of these weights as the final model. What this effectively does is move the convergence point from the boundaries of the low loss plateau to its center, which in turn widens the classification margin and improves generalization.</p><h2 id=bonus-depth-vs-width>Bonus: depth vs width</h2><p><em>The universal approximation theorem</em> states that a sufficiently wide neural network is able to approximate (with an arbitrary precision) any continuous well-behaved real function. Recently, there&rsquo;s been significant effort in the deep learning community to come up with an analogous statement, but this time relating the <em>depth</em> of the network to its approximation capacity. And, as it turns out, the efforts bear fruit: it can be shown that a sufficiently <em>deep</em> network with bounded width can also be considered an universal approximator. This is interesting, because it suggests that depth does not only affect the level of abstraction of features, but also the linear separability of those features (as does width).</p><h2 id=references>References</h2><p><a id=1>[1]</a>
Andrej Karpathy (2017)
<em>Software 2.0.</em>
<a href=https://karpathy.medium.com/software-2-0-a64152b37c35>https://karpathy.medium.com/software-2-0-a64152b37c35</a></p><p><a id=2>[2]</a>
Johanni Brea, Berfin Simsek, Bernd Illing, Wulfram Gerstner (2019)
<em>Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape.</em>
<a href=https://arxiv.org/abs/1907.02911>https://arxiv.org/abs/1907.02911</a></p><p><a id=3>[3]</a>
Ian J. Goodfellow, Oriol Vinyals, Andrew M. Saxe (2015)
<em>Qualitatively characterizing neural network optimization problems.</em>
<a href=https://arxiv.org/abs/1412.6544>https://arxiv.org/abs/1412.6544</a></p><p><a id=4>[4]</a>
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein (2018)
<em>Visualizing the Loss Landscape of Neural Nets.</em>
<a href=https://arxiv.org/abs/1712.09913>https://arxiv.org/abs/1712.09913</a></p><p><a id=5>[5]</a>
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson (2018)
<em>Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs</em>
<a href=https://arxiv.org/abs/1802.10026>https://arxiv.org/abs/1802.10026</a></p><p><a id=6>[6]</a>
Stanislav Fort, Stanislaw Jastrzebski (2019)
<em>Large Scale Structure of Neural Network Loss Landscapes.</em>
<a href=https://arxiv.org/abs/1906.04724>https://arxiv.org/abs/1906.04724</a></p><p><a id=7>[7]</a>
Gregory W. Benton, Wesley J. Maddox, Sanae Lotfi, Andrew Gordon Wilson (2021)
<em>Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling.</em>
<a href=https://arxiv.org/abs/2102.13042>https://arxiv.org/abs/2102.13042</a></p><p><a id=8>[8]</a>
Leo Diract (@leopd) (2019)
<em>Geometric Intuition for Training Neural Networks.</em>
<a href="https://www.youtube.com/watch?v=Z_MA8CWKxFU">https://www.youtube.com/watch?v=Z_MA8CWKxFU</a></p><p><a id=9>[9]</a>
Tom Goldstein (2018)
<em>What do neural loss surfaces look like?</em>
<a href="https://www.youtube.com/watch?v=78vq6kgsTa8">https://www.youtube.com/watch?v=78vq6kgsTa8</a></p><p><a id=10>[10]</a>
Tom Goldstein (2020)
<em>An empirical look at generalization in neural nets.</em>
<a href="https://www.youtube.com/watch?v=kcVWAKf7UAg">https://www.youtube.com/watch?v=kcVWAKf7UAg</a></p><p><a id=11>[11]</a>
Pratik Chaudhari, Stefano Soatto (2018)
<em>Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks.</em>
<a href=https://arxiv.org/abs/1710.11029>https://arxiv.org/abs/1710.11029</a></p><p><a id=12>[12]</a>
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson (2018)
<em>Averaging Weights Leads to Wider Optima and Better Generalization.</em>
<a href=https://arxiv.org/abs/1803.05407>https://arxiv.org/abs/1803.05407</a></p></div></article></main></div><footer class=footer><span class=footer_item></span>&nbsp;<div class=footer_social-icons><a href=https://www.linkedin.com/in/patryk-wegrzyn/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a href=https://twitter.com/_pwegrzyn target=_blank rel="noopener noreferrer me" title=X><svg viewBox="0 0 1200 1227" fill="currentcolor" xmlns="http://www.w3.org/2000/svg"><path d="M714.163 519.284 1160.89.0H1055.03L667.137 450.887 357.328.0H0L468.492 681.821.0 1226.37H105.866L515.491 750.218 842.672 1226.37H12e2L714.137 519.284H714.163zM569.165 687.828l-47.468-67.894L144.011 79.6944H306.615L611.412 515.685l47.468 67.894 396.2 566.721H892.476L569.165 687.854V687.828z"/></svg>
</a><a href=https://github.com/pwegrzyn target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></div><small class=footer_copyright>Â© 2024 Patryk Wegrzyn.
Powered by <a href=https://github.com/hugo-sid/hugo-blog-awesome target=_blank rel=noopener>Hugo blog awesome</a>.</small></footer><a href=# title="Go to top" id=totop><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" fill="currentcolor" stroke="currentcolor" viewBox="0 96 960 960"><path d="M283 704.739 234.261 656 480 410.261 725.739 656 677 704.739l-197-197-197 197z"/></svg>
</a><script src=https://pwegrzyn.github.io/js/main.min.35f435a5d8eac613c52daa28d8af544a4512337d3e95236e4a4978417b8dcb2f.js integrity="sha256-NfQ1pdjqxhPFLaoo2K9USkUSM30+lSNuSkl4QXuNyy8="></script></body></html>